{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH7qx_irU4Y8"
   },
   "source": [
    "###ML1_1: \n",
    "https://www.hackerrank.com/challenges/capturing-non-capturing-groups/problem?isFullScreen=true\n",
    "\n",
    "###ML1_2: \n",
    "https://www.hackerrank.com/challenges/branch-reset-groups/problem?isFullScreen=true\n",
    "\n",
    "###ML1_3: \n",
    "https://www.hackerrank.com/challenges/detect-html-links/problem?isFullScreen=true\n",
    "\n",
    "###ML1_4: Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML1_1\n",
    "(?:ok){3,}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xJfkstKpqsXp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The string S does not match the condition.\n"
     ]
    }
   ],
   "source": [
    "#ML2_1\n",
    "import re\n",
    "\n",
    "S = \"your_input_string\"\n",
    "pattern = r\"(\\d{2})(---|-|\\.|:)\\1\"\n",
    "match = re.search(pattern, S)\n",
    "\n",
    "if match:\n",
    "    print(\"The string S matches the condition.\")\n",
    "else:\n",
    "    print(\"The string S does not match the condition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML2_3\n",
    "import re\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n = int(input())\n",
    "    Regex = r'<a href=\"(.*?)\".*?>([\\w ,./]*)(?=</)'\n",
    "    for _ in range(n):\n",
    "        s = input()\n",
    "        links = re.findall(Regex, s)\n",
    "        for link, att in links:\n",
    "            print('%s,%s' % (link, att.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in e:\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: gensim in e:\\anaconda3\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: bokeh in e:\\anaconda3\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: umap-learn in e:\\anaconda3\\lib\\site-packages (0.5.5)\n",
      "Requirement already satisfied: click in e:\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in e:\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in e:\\anaconda3\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in e:\\anaconda3\\lib\\site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in e:\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: Jinja2>=2.9 in e:\\anaconda3\\lib\\site-packages (from bokeh) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.2 in e:\\anaconda3\\lib\\site-packages (from bokeh) (1.2.0)\n",
      "Requirement already satisfied: packaging>=16.8 in e:\\anaconda3\\lib\\site-packages (from bokeh) (23.1)\n",
      "Requirement already satisfied: pandas>=1.2 in e:\\anaconda3\\lib\\site-packages (from bokeh) (2.0.3)\n",
      "Requirement already satisfied: pillow>=7.1.0 in e:\\anaconda3\\lib\\site-packages (from bokeh) (10.0.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in e:\\anaconda3\\lib\\site-packages (from bokeh) (6.0)\n",
      "Requirement already satisfied: tornado>=6.2 in e:\\anaconda3\\lib\\site-packages (from bokeh) (6.3.2)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in e:\\anaconda3\\lib\\site-packages (from bokeh) (2022.9.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in e:\\anaconda3\\lib\\site-packages (from umap-learn) (1.3.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in e:\\anaconda3\\lib\\site-packages (from umap-learn) (0.57.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in e:\\anaconda3\\lib\\site-packages (from umap-learn) (0.5.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\anaconda3\\lib\\site-packages (from Jinja2>=2.9->bokeh) (2.1.1)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in e:\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn) (0.40.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\anaconda3\\lib\\site-packages (from pandas>=1.2->bokeh) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\anaconda3\\lib\\site-packages (from pandas>=1.2->bokeh) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in e:\\anaconda3\\lib\\site-packages (from pandas>=1.2->bokeh) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in e:\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22->umap-learn) (2.2.0)\n",
      "Requirement already satisfied: colorama in e:\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in e:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2->bokeh) (1.16.0)\n",
      "WARNING:tensorflow:From E:\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\albom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\albom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#ML1_4\n",
    "#Установка нужных пакетов\n",
    "!pip install --upgrade nltk gensim bokeh umap-learn\n",
    "\n",
    "import itertools\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96028\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Лемматизация\n",
    "df = pd.read_csv('E:/git_exercise/labeled.csv')\n",
    "#text_dataset = df.to_json(orient='records')\n",
    "\n",
    "#df['comment'] = df['comment'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x.split()])\n",
    "#df2 = df2.apply(lambda x: ' '.join(x for x in x.split()))\n",
    "df2=df['comment'].str.lower()\n",
    "\n",
    "# Объединение всех столбцов в одну строку\n",
    "text = ' '.join(df2.values.ravel())\n",
    "\n",
    "# Создание списка уникальных слов\n",
    "unique_words_before = np.unique(text.split())\n",
    "print(len(unique_words_before))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "#print(tokenizer.tokenize(' '.join(df2[50])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tok = []\n",
    "for x in df2:\n",
    "    data_tok.append(tokenizer.tokenize(' '.join(x).lower()))\n",
    "#checking\n",
    "\n",
    "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
    "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных слов до лемматизации: 96028\n",
      "Количество уникальных слов после лемматизации: 96008\n"
     ]
    }
   ],
   "source": [
    "# Лемматизация каждого слова в списке\n",
    "#lemmatized_words = [[lemmatizer.lemmatize(word) for word in sublist] for sublist in df2]\n",
    "#lemmatized_words = [lemmatizer.lemmatize(' '.join(word).lower()) for word in df2]\n",
    "lemmatized_words=[lemmatizer.lemmatize(word) for word in unique_words_before]\n",
    "# Создание списка уникальных слов после лемматизации\n",
    "#unique_words_after = set(word for sublist in lemmatized_words for word in sublist)\n",
    "unique_words_after = set(lemmatized_words)\n",
    "# Подсчет количества уникальных слов\n",
    "print(\"Количество уникальных слов до лемматизации:\", len(unique_words_before))\n",
    "print(\"Количество уникальных слов после лемматизации:\", len(unique_words_after))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Стемминг\n",
    "#df2 = df2.apply(lambda x: [ps.stem(word) for word in x])\n",
    "#df2[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных слов до стемминга: 96028\n",
      "Количество уникальных слов после стемминга: 96026\n"
     ]
    }
   ],
   "source": [
    "stemmed_words = [ps.stem(' '.join(word).lower()) for word in unique_words_before]\n",
    "\n",
    "# Создание списка уникальных слов после стемминга\n",
    "unique_words_after = set(stemmed_words)\n",
    "\n",
    "# Подсчет количества уникальных слов\n",
    "print(\"Количество уникальных слов до стемминга:\", len(unique_words_before))\n",
    "print(\"Количество уникальных слов после стемминга:\", len(unique_words_after))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'в': 82636,\n",
       " 'е': 168778,\n",
       " 'р': 93123,\n",
       " 'б': 36164,\n",
       " 'л': 81460,\n",
       " 'ю': 13973,\n",
       " 'д': 61425,\n",
       " 'о': 222995,\n",
       " '-': 8035,\n",
       " 'т': 147967,\n",
       " ' ': 388347,\n",
       " 'з': 32134,\n",
       " 'а': 167895,\n",
       " 'ч': 31424,\n",
       " '?': 4882,\n",
       " 'и': 141776,\n",
       " 'ы': 37667,\n",
       " ',': 34386,\n",
       " '.': 31870,\n",
       " '\\n': 13444,\n",
       " 'х': 20086,\n",
       " 'э': 8793,\n",
       " 'у': 56704,\n",
       " 'ш': 14886,\n",
       " 'н': 130546,\n",
       " 'к': 74658,\n",
       " 'г': 30133,\n",
       " 'с': 101968,\n",
       " 'я': 37714,\n",
       " 'м': 63705,\n",
       " 'щ': 6703,\n",
       " 'ж': 20545,\n",
       " 'ь': 37965,\n",
       " 'п': 61133,\n",
       " 'ц': 8351,\n",
       " 'й': 21869,\n",
       " 'ф': 5993,\n",
       " '6': 732,\n",
       " ')': 2939,\n",
       " '!': 2096,\n",
       " ':': 1038,\n",
       " 'ё': 3838,\n",
       " 'ъ': 610,\n",
       " 'p': 478,\n",
       " 'l': 495,\n",
       " 'a': 1031,\n",
       " 'n': 665,\n",
       " 'e': 1077,\n",
       " 't': 696,\n",
       " 'r': 728,\n",
       " 'u': 359,\n",
       " 'h': 311,\n",
       " 's': 804,\n",
       " 'i': 864,\n",
       " 'k': 199,\n",
       " 'b': 360,\n",
       " 'o': 888,\n",
       " 'y': 209,\n",
       " 'c': 534,\n",
       " 'd': 508,\n",
       " 'z': 54,\n",
       " '4': 1067,\n",
       " '9': 797,\n",
       " '3': 1408,\n",
       " '2': 2428,\n",
       " '0': 5087,\n",
       " '8': 770,\n",
       " 'v': 177,\n",
       " '5': 1646,\n",
       " 'f': 272,\n",
       " 'm': 429,\n",
       " 'w': 219,\n",
       " '1': 3102,\n",
       " '7': 657,\n",
       " 'q': 31,\n",
       " 'g': 296,\n",
       " 'j': 60,\n",
       " '(': 1779,\n",
       " 'x': 145,\n",
       " ';': 74}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = {}\n",
    "for line in df2:\n",
    "    for word in line:\n",
    "        if word not in bag_of_words:\n",
    "            bag_of_words[word] = 1\n",
    "        else:\n",
    "            bag_of_words[word] += 1\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP1_homework",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
